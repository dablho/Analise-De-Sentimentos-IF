import streamlit as st
import pandas as pd
import psycopg2
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import string
import unicodedata
from collections import Counter, defaultdict
import os
import networkx as nx
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('stopwords')
nltk.download('punkt')

# Definir o diret√≥rio para armazenar os dados do NLTK
nltk_data_dir = os.path.join(os.getcwd(), 'nltk_data')
if not os.path.exists(nltk_data_dir):
    os.makedirs(nltk_data_dir)

# Adicionar o diret√≥rio ao caminho de dados do NLTK
nltk.data.path.append(nltk_data_dir)
nltk.download('all', download_dir=nltk_data_dir)

# image1 = "imagens/Sentimentos.jpg"
# image2 = "imagens/Sentimentos_Percentual.jpg"
# st.image(image1, width=300)
# st.image(image2, width=300)

# Configura√ß√£o da p√°gina
st.set_page_config(page_title="An√°lise de Sentimentos", layout="centered")

@st.cache_resource
# Conex√£o com o banco de dados
def get_data_from_db():
    try:
        conn = psycopg2.connect(
             host=os.getenv("host"),
             database=os.getenv("database"),
             user=os.getenv("user"),
             password=os.getenv("password")
        )
        query = "SELECT * FROM prova.tabela_tcc"
        df = pd.read_sql(query, conn)
        conn.close()
        return df
    except Exception as e:
        st.error(f"Erro ao conectar ao banco de dados: {e}")
        return None


def generate_wordcloud(texts):
    # Combina todos os textos em um √∫nico string
    text = " ".join(texts)

    # Cria a wordcloud sem pr√©-processamento
    wordcloud = WordCloud(
        width=800, height=400, background_color="white", max_words=200
    ).generate(text)

    return wordcloud


def get_word_frequency(texts, top_n=10):
    # Junta todos os textos
    all_words = " ".join(texts).split()

    # Conta a frequ√™ncia das palavras
    word_freq = Counter(all_words)

    # Pega as top N palavras mais frequentes
    top_words = dict(
        sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:top_n]
    )

    return top_words


def get_processed_word_frequency(texts, top_n=10):
    # Pr√©-processa e tokeniza todos os textos
    processed_words = []
    for text in texts:
        processed_text = preprocess_text_for_wordcloud(text)
        processed_words.extend(processed_text.split())

    # Conta a frequ√™ncia das palavras
    word_freq = Counter(processed_words)

    # Pega as top N palavras mais frequentes
    top_words = dict(
        sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:top_n]
    )

    return top_words


# Pr√©-processamento de texto
def preprocess_text(text):
    # Remove acentos
    text = "".join(
        c for c in unicodedata.normalize("NFD", text) if unicodedata.category(c) != "Mn"
    )
    # Converte para min√∫sculas
    text = text.lower()
    # Remove pontua√ß√£o
    text = text.translate(str.maketrans("", "", string.punctuation))
    return text


def preprocess_text_for_wordcloud(text):
    # Remove acentos
    text = "".join(
        c for c in unicodedata.normalize("NFD", text) if unicodedata.category(c) != "Mn"
    )

    # Converte para min√∫sculas
    text = text.lower()

    # Remove pontua√ß√£o
    text = text.translate(str.maketrans("", "", string.punctuation))

    # Tokeniza√ß√£o
    tokens = word_tokenize(text)

    # Remove stopwords
    stop_words = set(stopwords.words("portuguese"))
    tokens = [word for word in tokens if word not in stop_words]

    return " ".join(tokens)


def generate_processed_wordcloud(texts):
    # Pr√©-processa todos os textos
    processed_texts = [preprocess_text_for_wordcloud(text) for text in texts]

    # Combina todos os textos processados
    text = " ".join(processed_texts)

    # Cria a wordcloud
    wordcloud = WordCloud(
        width=800,
        height=400,
        background_color="white",
        max_words=200,
        colormap="viridis",
        prefer_horizontal=0.7,
    ).generate(text)

    return wordcloud


##############


def get_tetragrams(texts, top_n=10):
    # Pr√©-processa e obt√©m os tetragramas
    tetragrams = []
    for text in texts:
        # Pr√©-processamento
        processed_text = preprocess_text_with_custom_stopwords(text)
        words = processed_text.split()

        # Gera tetragramas
        for i in range(len(words) - 3):
            tetragrams.append(tuple(words[i : i + 4]))

    # Conta frequ√™ncia
    tetragrams_freq = Counter(tetragrams)

    # Retorna os top N mais frequentes
    return dict(
        sorted(tetragrams_freq.items(), key=lambda x: x[1], reverse=True)[:top_n]
    )


def create_tetragrams_graph(texts):
    # Obt√©m os tetragramas mais frequentes
    top_tetragrams = get_tetragrams(texts)

    # Cria o grafo
    G = nx.DiGraph()

    # Adiciona n√≥s e arestas
    for tetagram, weight in top_tetragrams.items():
        for i in range(3):
            G.add_edge(tetagram[i], tetagram[i + 1], weight=weight)

    return G, top_tetragrams


###############


def preprocess_text_with_custom_stopwords(text):
    # Stopwords personalizadas
    custom_stopwords = {"ja", "so", "pra"}

    # Pega as stopwords padr√£o do portugu√™s
    stop_words = set(stopwords.words("portuguese"))

    # Adiciona as stopwords personalizadas
    stop_words.update(custom_stopwords)

    # Remove acentos
    text = "".join(
        c for c in unicodedata.normalize("NFD", text) if unicodedata.category(c) != "Mn"
    )

    # Converte para min√∫sculas
    text = text.lower()

    # Remove pontua√ß√£o
    text = text.translate(str.maketrans("", "", string.punctuation))

    # Tokeniza√ß√£o
    tokens = word_tokenize(text)

    # Remove stopwords (incluindo as personalizadas)
    tokens = [word for word in tokens if word not in stop_words]

    return " ".join(tokens)


def generate_custom_wordcloud(texts):
    # Pr√©-processa todos os textos
    processed_texts = [preprocess_text_with_custom_stopwords(text) for text in texts]

    # Combina todos os textos processados
    text = " ".join(processed_texts)

    # Cria a wordcloud
    wordcloud = WordCloud(
        width=800,
        height=400,
        background_color="white",
        max_words=200,
        colormap="viridis",
        prefer_horizontal=0.7,
    ).generate(text)

    return wordcloud


def get_custom_word_frequency(texts, top_n=10):
    # Pr√©-processa e tokeniza todos os textos
    processed_words = []
    for text in texts:
        processed_text = preprocess_text_with_custom_stopwords(text)
        processed_words.extend(processed_text.split())

    # Conta a frequ√™ncia das palavras
    word_freq = Counter(processed_words)

    # Pega as top N palavras mais frequentes
    top_words = dict(
        sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:top_n]
    )

    return top_words


# An√°lise de aspectos
def analyze_aspects(text):
    aspects = {
        "App": ["app", "aplicativo", "tela", "interface", "navega√ß√£o", "bug"],
        "Cr√©dito": ["credito", "cr√©dito", "limite", "emprestimo", "score"],
        "Atendimento": ["atendimento", "suporte", "chat", "ajuda", "duvida"],
        "Transfer√™ncia": ["transferencia", "pix", "ted", "pagamento", "saldo"],
        "Taxas": ["taxa", "tarifa", "custo", "cobran√ßa", "juros"],
        "Seguran√ßa": ["seguran√ßa", "fraude", "golpe", "senha", "bloqueio"],
        "Conta": ["conta", "digital", "cart√£o", "cadastro", "abertura"],
    }

    text_lower = text.lower()
    found_aspects = defaultdict(int)

    for aspect, keywords in aspects.items():
        for keyword in keywords:
            if keyword in text_lower:
                found_aspects[aspect] += 1

    return dict(found_aspects) if found_aspects else {"Geral": 1}


def main():
    st.title("An√°lise de Sentimentos - Coment√°rios de Institui√ß√µes Financeiras")

    # Carrega os dados
    with st.spinner("Carregando dados..."):
        data = get_data_from_db()

    if data is None:
        st.error("N√£o foi poss√≠vel carregar os dados.")
        return

    st.write("Aqui est√£o os dados extra√≠dos do banco de dados:")
    st.write(data)


    # An√°lise de aspectos
    with st.spinner("Analisando aspectos..."):
        data["aspects"] = [analyze_aspects(comment) for comment in data["comentario"]]

    # Container centralizado para os gr√°ficos
    st.write("## Visualiza√ß√µes dos Resultados")

    # Nuvem de Palavras
    st.write("")
    st.subheader("Nuvem de Palavras dos Coment√°rios")

    # Gera a nuvem de palavras
    wordcloud = generate_wordcloud(data["comentario"])

    # Cria e exibe o gr√°fico
    fig_wordcloud = plt.figure(figsize=(10, 6))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    plt.tight_layout(pad=0)
    st.pyplot(fig_wordcloud)

    # Gr√°fico de Frequ√™ncia de Palavras
    st.write("")
    st.subheader("Top 10 Palavras Mais Frequentes")

    # Obt√©m as palavras mais frequentes
    top_words = get_word_frequency(data["comentario"])

    # Cria o gr√°fico de barras
    fig_freq = plt.figure(figsize=(12, 6))
    plt.bar(top_words.keys(), top_words.values(), color="skyblue")
    plt.title("Frequ√™ncia das Palavras Mais Comuns")
    plt.xlabel("Palavras")
    plt.ylabel("Frequ√™ncia")
    plt.xticks(rotation=45, ha="right")

    # Adiciona os valores sobre as barras
    for i, (word, freq) in enumerate(top_words.items()):
        plt.text(i, freq, str(freq), ha="center", va="bottom")

    plt.tight_layout()
    st.pyplot(fig_freq)

    # Nuvem de Palavras com Pr√©-processamento
    st.write("")
    st.subheader("Nuvem de Palavras dos Coment√°rios (Com remo√ß√£o de Stop Words)")

    # Gera a nuvem de palavras processada
    wordcloud_processed = generate_processed_wordcloud(data["comentario"])

    # Cria e exibe o gr√°fico
    fig_wordcloud_processed = plt.figure(figsize=(10, 6))
    plt.imshow(wordcloud_processed, interpolation="bilinear")
    plt.axis("off")
    plt.tight_layout(pad=0)
    st.pyplot(fig_wordcloud_processed)

    # Gr√°fico de Frequ√™ncia de Palavras Processadas
    st.write("")
    st.subheader("Top 10 Palavras Mais Frequentes (Com remo√ß√£o de Stop Words)")

    # Obt√©m as palavras mais frequentes processadas
    top_words_processed = get_processed_word_frequency(data["comentario"])

    # Cria o gr√°fico de barras
    fig_freq_processed = plt.figure(figsize=(12, 6))
    plt.bar(
        top_words_processed.keys(), top_words_processed.values(), color="lightgreen"
    )
    plt.title("Frequ√™ncia das Palavras Mais Comuns (Com remo√ß√£o de Stop Words)")
    plt.xlabel("Palavras")
    plt.ylabel("Frequ√™ncia")
    plt.xticks(rotation=45, ha="right")

    # Adiciona os valores sobre as barras
    for i, (word, freq) in enumerate(top_words_processed.items()):
        plt.text(i, freq, str(freq), ha="center", va="bottom")

    plt.tight_layout()
    st.pyplot(fig_freq_processed)

    st.write("")

    ####

    # Nuvem de Palavras com Stopwords Personalizadas
    st.write("")
    st.subheader("Nuvem de Palavras dos Coment√°rios (Com Stopwords Personalizadas)")

    # Gera a nuvem de palavras
    wordcloud_custom = generate_custom_wordcloud(data["comentario"])

    # Cria e exibe o gr√°fico
    fig_wordcloud_custom = plt.figure(figsize=(10, 6))
    plt.imshow(wordcloud_custom, interpolation="bilinear")
    plt.axis("off")
    plt.tight_layout(pad=0)
    st.pyplot(fig_wordcloud_custom)

    # Gr√°fico de Frequ√™ncia de Palavras
    st.write("")
    st.subheader("Top 10 Palavras Mais Frequentes (Com Stopwords Personalizadas)")

    # Obt√©m as palavras mais frequentes
    top_words_custom = get_custom_word_frequency(data["comentario"])

    # Cria o gr√°fico de barras
    fig_freq_custom = plt.figure(figsize=(12, 6))
    plt.bar(top_words_custom.keys(), top_words_custom.values(), color="lightgreen")
    plt.title("Frequ√™ncia das Palavras Mais Comuns")
    plt.xlabel("Palavras")
    plt.ylabel("Frequ√™ncia")
    plt.xticks(rotation=45, ha="right")

    # Adiciona os valores sobre as barras
    for i, (word, freq) in enumerate(top_words_custom.items()):
        plt.text(i, freq, str(freq), ha="center", va="bottom")

    plt.tight_layout()
    st.pyplot(fig_freq_custom)


    # Grafo de Tetragramas
    st.write("")
    st.subheader("Grafo de Tetragramas Mais Frequentes")

    # Cria o grafo
    G, top_tetragrams = create_tetragrams_graph(data["comentario"])

    # Configura o layout do grafo
    pos = nx.spring_layout(G, k=3, seed=20, iterations=50)

    # Cria a figura
    fig_graph = plt.figure(figsize=(15, 10))

    # Desenha as arestas com largura baseada no peso
    edge_weights = [G[u][v]["weight"] for u, v in G.edges()]
    nx.draw_networkx_edges(
        G,
        pos,
        edge_color="gray",
        width=[w / max(edge_weights) * 3 for w in edge_weights],
        alpha=0.5,
        arrows=True,
        arrowsize=20,
    )

    # Desenha os n√≥s
    nx.draw_networkx_nodes(G, pos, node_color="lightblue", node_size=2000, alpha=0.7)

    # Adiciona labels aos n√≥s
    nx.draw_networkx_labels(G, pos, font_size=12)

    plt.title("Conex√µes entre Palavras Frequentes nos Coment√°rios", fontsize=20)
    plt.axis("off")
    plt.tight_layout()

    # Mostra o grafo
    st.pyplot(fig_graph)

    st.write("")
    st.subheader("Frequ√™ncia dos Tetragramas Mais Comuns")

    top_tetragrams_bar = get_tetragrams(data["comentario"], top_n=20)

    # Prepara e ordena os dados para o gr√°fico
    sorted_tetragrams = sorted(
        top_tetragrams_bar.items(), key=lambda item: item[1]
    )  # Ordena√ß√£o crescente
    tetagram_labels = [" ‚Üí ".join(tetagram) for tetagram, _ in sorted_tetragrams]
    frequencies = [freq for _, freq in sorted_tetragrams]

    # Cria o gr√°fico de barras horizontais
    fig_tetagram = plt.figure(figsize=(10, 8))
    bars = plt.barh(range(len(frequencies)), frequencies, color="lightblue")

    # Configura√ß√µes do gr√°fico
    plt.title(
        "Tetragramas Mais Frequentes nos Coment√°rios", fontsize=16
    )  # Tamanho do t√≠tulo ajustado
    plt.xlabel("Frequ√™ncia", fontsize=12)
    plt.ylabel("Sequ√™ncia de Palavras", fontsize=12)

    # Configura os r√≥tulos do eixo Y
    plt.yticks(range(len(tetagram_labels)), tetagram_labels, fontsize=10)

    # Adiciona os valores ao lado das barras
    for bar in bars:
        width = bar.get_width()
        plt.text(
            width,
            bar.get_y() + bar.get_height() / 2.0,
            f"{int(width)}",
            ha="left",
            va="center",
            fontsize=10,
        )

    # Ajusta o layout para evitar cortes
    plt.tight_layout()

    # Mostra o gr√°fico
    st.pyplot(fig_tetagram)


    ####



    



    ###











    st.write("")
    st.subheader("An√°lise de Sentimentos dos Coment√°rios")



    # Cria DataFrame com an√°lise detalhada
    
    



    # Cria DataFrame com a an√°lise




    # Aplica estilo para destacar os sentimentos com cores
    



    

    # Aplica o estilo e mostra a tabela






    # Adiciona filtros




    # Aplica filtros





    # Ordena por score selecionado



    ######

    st.write("")
    # Primeiro gr√°fico - Distribui√ß√£o de Sentimentos
    st.image("imagens/Sentimentos.jpg", width=800)
    #st.image("Sentimentos.jpg", width=800)


    # Espa√ßo entre os gr√°ficos
    st.write("")

    # Segundo gr√°fico - Distribui√ß√£o de Scores
    st.image("imagens/Sentimentos_Percentual.jpg", width=800)
    #st.image("Sentimentos_Percentual.jpg", width=800)


    ## tabela sentimentos

    st.subheader("Coment√°rios e Seus Sentimentos")
    
    df2 = pd.read_csv('resultados_sentimentos2.csv')
    st.dataframe(df2)




        # Adiciona uma se√ß√£o de exemplos de coment√°rios
    st.write("")  # Adiciona um espa√ßo
    st.subheader("Exemplos de Coment√°rios por Sentimento")

    # Cria tr√™s colunas para organizar os exemplos
    col1, col2, col3 = st.columns(3)

    # Exemplos de coment√°rios positivos
    with col1:
        st.write("### üü¢ Coment√°rios Positivos")
        positivos = df2[df2['sentimento'] == 'Positivo'].nlargest(3, 'score')
        for _, row in positivos.iterrows():
            st.write(f"**Score: {row['score']:.2f}**")
            st.write(f"_{row['texto_original']}_")
            st.write("---")

    # Exemplos de coment√°rios neutros
    with col2:
        st.write("### ‚ö™ Coment√°rios Neutros")
        neutros = df2[df2['sentimento'] == 'Neutro'].nlargest(3, 'score')
        for _, row in neutros.iterrows():
            st.write(f"**Score: {row['score']:.2f}**")
            st.write(f"_{row['texto_original']}_")
            st.write("---")

    # Exemplos de coment√°rios negativos
    with col3:
        st.write("### üî¥ Coment√°rios Negativos")
        negativos = df2[df2['sentimento'] == 'Negativo'].nlargest(3, 'score')
        for _, row in negativos.iterrows():
            st.write(f"**Score: {row['score']:.2f}**")
            st.write(f"_{row['texto_original']}_")
            st.write("---")


    # Espa√ßo entre os gr√°ficos
    st.write("")

    # Terceiro gr√°fico - An√°lise de Aspectos
    st.subheader("An√°lise de Aspectos")
    all_aspects = defaultdict(int)
    for aspects in data["aspects"]:
        for aspect, count in aspects.items():
            all_aspects[aspect] += count

    fig3 = plt.figure(figsize=(10, 6))
    aspect_items = sorted(all_aspects.items(), key=lambda x: x[1], reverse=True)
    aspects, counts = zip(*aspect_items)

    plt.bar(aspects, counts, color="lightblue")
    plt.title("Aspectos Mencionados nos Coment√°rios")
    plt.xlabel("Aspectos")
    plt.ylabel("N√∫mero de Men√ß√µes")
    plt.xticks(rotation=45)

    # Adiciona os valores sobre as barras
    for i, v in enumerate(counts):
        plt.text(i, v, str(v), ha="center", va="bottom")

    plt.tight_layout()
    st.pyplot(fig3)


    # Estat√≠sticas gerais
    #total_comments, positive_perc, negative_perc, neutral_perc

    st.write("")



    # Exemplos de coment√°rios



if __name__ == "__main__":
    main()
